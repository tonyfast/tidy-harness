{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# `tidy-harness`\n",
    "\n",
    "A _tidy_ `pandas.DataFrame` with `scikit-learn` models, interactive `bokeh` visualizations, and `jinja2` templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "### Example: Modeling Fisher's ðŸŒ¸ Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `model_selection.ShuffleSplit` not found.\n"
     ]
    }
   ],
   "source": [
    "from harness import Harness\n",
    "from pandas import Categorical\n",
    "from sklearn import datasets, discriminant_analysis\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Harness is just a dataframe\n",
    "df = Harness(\n",
    "    data=iris['data'], index=Categorical(iris['target']),\n",
    "    estimator=discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    feature_level=-1,\n",
    ")\n",
    "\n",
    "# Fit the model with 50 random rows.\n",
    "df.sample(50).fit()\n",
    "\n",
    "# Transform the dataframe\n",
    "transformed = df.transform()\n",
    "transformed.set_index(\n",
    "    df.index\n",
    "    .rename_categories(iris['target_names'])\n",
    "    .rename('species'), append=True, inplace=True,\n",
    ")\n",
    "\n",
    "# Plot the dataframe using Bokeh charts.\n",
    "with transformed.reset_index().DataSource(x=0, y=1) as source:\n",
    "    source.Scatter(color='species')\n",
    "    source.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Background\n",
    "\n",
    "`harness` initially responded to the need for `scikit-learn` models closer to a `pandas.DataFrame`.  Since a DataFrame is __[Tidy Data](http://vita.had.co.nz/papers/tidy-data.pdf)__ the rows and columns can assist in tracking samples and features over many estimations.\n",
    "\n",
    "The `DataFrame` has a powerful declarative syntax, consider the `groupby` and `rolling` apis.  There is a modern tendency toward declarative and functional syntaxes in scientific computing and visualization.  This is observed in [altair](https://github.com/altair-viz/altair), dask, and scikit-learn.\n",
    "\n",
    "`tidy-harness` aims to provide a chain interface between `pandas.DataFrame` objects and other popular scientific computing libraries in the python ecosystem.  The initial `harness` extensions :\n",
    "\n",
    "* attach a `scikit-learn` estimator to the dataframe.\n",
    "* attach a shared `jinja2` environment to render narratives about the dataframes.\n",
    "* `bokeh` plotting methods with a `contextmanager` for interactive visualization development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development\n",
    "\n",
    "> The development scripts can be run through this notebook.\n",
    "\n",
    "Jupyter notebooks are used for all Python development in this project.  The key features are:\n",
    "\n",
    "* [`watchdog`]() file system watcher that converts notebooks to python scripts with `nbconvert`.  _Tests are not converted._\n",
    "* [`nbconvert`]() with the `--execute` flag to run notebooks and fill out their output.  _The current goal is for the notebook to be viewable in a Github repo.\n",
    "* [`pytest-ipynb`]() to run tests directly on the notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the python module\n",
    "\n",
    "The script below:\n",
    "\n",
    "* Installs a develop copy of `harness`\n",
    "* Listens for file systems events to convert notebooks to `python` scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job # 3 in a separate thread.\n"
     ]
    }
   ],
   "source": [
    "%%script bash --bg\n",
    "python setup.py develop\n",
    "watchmedo tricks tricks.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All background processes were killed.\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell to stop watching the files\n",
    "%killbgscripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Run Tests\n",
    "\n",
    "The tests require `pytest` and `pytest-ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.5.2, pytest-2.9.2, py-1.4.31, pluggy-0.3.1\n",
      "rootdir: /Users/tonyfast/tidy-harness, inifile: \n",
      "plugins: hypothesis-3.5.3, flake8-0.7, ipynb-1.1.0\n",
      "collected 40 items\n",
      "\n",
      "tests/test Can Harness Add a Networkx extension.ipynb ........\n",
      "tests/test Does Harness Write Declarative Model Pipelines.ipynb .............\n",
      "tests/test Does Harness work for the iris dataset emoji style.ipynb ..........\n",
      "tests/test Does Harness work for the iris dataset.ipynb .........\n",
      "\n",
      "========================== 40 passed in 20.81 seconds ==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook tests/What are the UML diagrams.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 278936 bytes to tests/What are the UML diagrams.ipynb\n",
      "[NbConvertApp] Converting notebook tests/test Can Harness Add a Networkx extension.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 40306 bytes to tests/test Can Harness Add a Networkx extension.ipynb\n",
      "[NbConvertApp] Converting notebook tests/test Does Harness Write Declarative Model Pipelines.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 18413 bytes to tests/test Does Harness Write Declarative Model Pipelines.ipynb\n",
      "[NbConvertApp] Converting notebook tests/test Does Harness work for the iris dataset emoji style.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 14266 bytes to tests/test Does Harness work for the iris dataset emoji style.ipynb\n",
      "[NbConvertApp] Converting notebook tests/test Does Harness work for the iris dataset.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 13532 bytes to tests/test Does Harness work for the iris dataset.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "jupyter nbconvert tests/*.ipynb --execute --to notebook --inplace \n",
    "py.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
